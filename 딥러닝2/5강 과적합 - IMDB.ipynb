{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b63227c",
   "metadata": {},
   "source": [
    "# 과소적합 (underfitting)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9485df",
   "metadata": {},
   "source": [
    "우리가 공부했던 과적합을 실험적으로 구현해보겠습니다.  \n",
    "이진분류에서 다뤘던 신경망과 IMDB 데이터셋으로요.  \n",
    "![](https://drive.google.com/thumbnail?id=1qyTwfr1hPiFzpAflIPrlbjykCY9XGUT3&sz=s4000)\n",
    "\n",
    "---\n",
    "앞에서처럼 훈련 데이터를 직접 나눌수도 있지만 [fit](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential#fit)메서드에서 validation_split 설정으로 편리하게 나눠서 학습시킬수 있습니다.  \n",
    "0과 1사의 값으로 검증데이터의 비율을 지정해줍니다.  \n",
    "입력 뉴런의 개수는 데이터가 입력될때 결정됩니다.  \n",
    "`input_shape`을 정의하지 않은 경우는 첫번째 데이터가 입력될때 그 데이터의 피처수로 입력 뉴런수가 결정이 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0294fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from keras.datasets import imdb\n",
    "from keras import models\n",
    "from keras.layers import Dense\n",
    "from tensorflow.keras.utils import plot_model\n",
    "import numpy as np\n",
    "\n",
    "(train_data, train_labels), _ = imdb.load_data(num_words=10000)\n",
    "\n",
    "def vectorize_sequences(sequences, dimension=10000):\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1.\n",
    "    return results\n",
    "\n",
    "train_data = vectorize_sequences(train_data)\n",
    "\n",
    "original_model = keras.Sequential([\n",
    "    Dense(16, activation=\"relu\"),\n",
    "    Dense(16, activation=\"relu\"),\n",
    "    Dense(1, activation=\"sigmoid\")])\n",
    "\n",
    "original_model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "original_hist = original_model.fit(train_data, train_labels,\n",
    "                             epochs=20, batch_size=512, validation_split=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318adc8e",
   "metadata": {},
   "source": [
    "은닉층의 뉴런수를 16에서 4로 줄인 작은 신경망을 만들고 학습시켜 보겠습니다.  \n",
    "![](https://drive.google.com/thumbnail?id=1ej6ZYCbe5_Lc8HNAVxcpEkkfV_9tUO8a&sz=s4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caadc333",
   "metadata": {},
   "outputs": [],
   "source": [
    "smaller_model = keras.Sequential([\n",
    "    Dense(4, activation=\"relu\"),\n",
    "    Dense(4, activation=\"relu\"),\n",
    "    Dense(1, activation=\"sigmoid\")])\n",
    "\n",
    "smaller_model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "smaller_model_hist = smaller_model.fit(\n",
    "    train_data, train_labels,\n",
    "    epochs=20, batch_size=512, validation_split=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09efadd9",
   "metadata": {},
   "source": [
    "작은 모델의 파라미터 개수가 중간모델의 4분의 1밖에 안됩니다.  \n",
    "훈련데이터의 정보를 저장할 수 있는 용량이 4분의 1밖에 안된다는 뜻이지요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7755ecbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_model.summary()\n",
    "smaller_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9501ccec",
   "metadata": {},
   "source": [
    "훈련데이터에 대해서는 중간 신경망의 손실함수값이 작은 신경망보다 빠르게 감소합니다.  \n",
    "중간 신경망의 메모리가 더 커서 훈련 데이터의 정보를 빠르고 많이 습득하기 때문이겠죠."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32890b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "epochs = range(1, 21)\n",
    "original_train_loss = original_hist.history['loss']\n",
    "smaller_model_train_loss = smaller_model_hist.history['loss']\n",
    "\n",
    "plt.plot(epochs, original_train_loss, 'b+', label='Original model')\n",
    "plt.plot(epochs, smaller_model_train_loss, 'bo', label='Smaller model')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Training loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42350361",
   "metadata": {},
   "source": [
    "검증데이터에 대해서는 작은 신경망에서 과적합이 더 늦게 그리고 더 완만히 진행됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959ae806",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "original_val_loss = original_hist.history['val_loss']\n",
    "smaller_model_val_loss = smaller_model_hist.history['val_loss']\n",
    "\n",
    "plt.plot(epochs, original_val_loss, 'b+', label='Original model')\n",
    "plt.plot(epochs, smaller_model_val_loss, 'bo', label='Smaller model')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e81601c",
   "metadata": {},
   "source": [
    "# 과대적합 (overfitting)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7579f6be",
   "metadata": {},
   "source": [
    "은닉층의 뉴런수를 16에서 512로 늘린 큰 신경망을 만들고 학습시켜 보겠습니다.  \n",
    "![](https://drive.google.com/thumbnail?id=1q0daZATKf8vTpHRtPelk0bcu9_CcgbLz&sz=s4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f374e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigger_model = keras.Sequential([\n",
    "    Dense(512, activation=\"relu\"),\n",
    "    Dense(512, activation=\"relu\"),\n",
    "    Dense(1, activation=\"sigmoid\")])\n",
    "\n",
    "bigger_model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "bigger_model_hist = bigger_model.fit(\n",
    "    train_data, train_labels,\n",
    "    epochs=20, batch_size=512, validation_split=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33da91b",
   "metadata": {},
   "source": [
    "큰 모델의 파라미터 개수가 중간모델의 무려 33배나 됩니다.  \n",
    "기억 용량도 33배라는 뜻이지요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4d9973",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_model.summary()\n",
    "bigger_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3419bed3",
   "metadata": {},
   "source": [
    "훈련데이터에 대해서는 큰 신경망의 손실함수값이 빠르게 0으로 떨어집니다.  \n",
    "그런데 학습 중간에 튀는 불안정한 현상을 볼수 있네요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1027e1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_train_loss = original_hist.history['loss']\n",
    "bigger_model_train_loss = bigger_model_hist.history['loss']\n",
    "\n",
    "plt.plot(epochs, original_train_loss, 'b+', label='Original model')\n",
    "plt.plot(epochs, bigger_model_train_loss, 'bo', label='Bigger model')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Training loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c8586a",
   "metadata": {},
   "source": [
    "검증데이터에 대해서는 큰 신경망에서 과적합이 더 빨리 그리고 더 가파르게 그리고 더 불안정하게 진행됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967a4c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigger_model_val_loss = bigger_model_hist.history['val_loss']\n",
    "\n",
    "plt.plot(epochs, original_val_loss, 'b+', label='Original model')\n",
    "plt.plot(epochs, bigger_model_val_loss, 'bo', label='Bigger model')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89164b1",
   "metadata": {},
   "source": [
    "# $L^2$-규제 ($L^2$-regularization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8238aab3",
   "metadata": {},
   "source": [
    "우리는 $L^2$-규제에 대해 공부했습니다.  \n",
    "기존 손실함수에 $L^2$ 페널티 ${1 \\over 2} \\times \\lambda \\times \\sum w_{i,j}^2$를 더합니다.  \n",
    "여기서 $w_{i,j}$는 모든 가중치를 뜻합니다.  \n",
    "가중치 감소 계수 $\\lambda$는 직접 설정해야 하는 하이퍼 파라미터입니다.  \n",
    "크게 잡으면 페널티가 큰 역할을 하고 작게 잡으면 페널티 역할이 작아집니다.  \n",
    "미분은 선형이기 때문에 새로운 손실함수의 그레디언트는 기존 손실함수의 그레디언트와 페널티의 그레디언트를 합한 것과 같습니다.  \n",
    "경사하강법을 적용하면 각 그레디언트의 반대방향의 합으로 가중치를 업데이트합니다.  \n",
    "각각은 정반대 역할을 합니다.  \n",
    "기존 손실함수의 그레디언트의 반대방향으로 업데이트하면 신경망은 훈련데이터의 정보를 더 담아냅니다.  \n",
    "$L^2$-페널티의 그레디언트의 반대방향 업데이트는 신경망의 가중치를 비슷하게 만드려는 경향이 있습니다.  \n",
    "이는 습득한 정보를 뭉게버리는 효과가 있습니다.  \n",
    "케라스에서는 [tf.keras.regularizers.L2](https://www.tensorflow.org/api_docs/python/tf/keras/regularizers/L2)를 통해 구현되어 있습니다.  \n",
    "[Dense](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense) 클래스에서 인수로 설정할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbaae1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import regularizers\n",
    "\n",
    "l2_model = keras.Sequential([\n",
    "    Dense(16, kernel_regularizer=regularizers.l2(0.002), activation=\"relu\"),\n",
    "    Dense(16, kernel_regularizer=regularizers.l2(0.002), activation=\"relu\"),\n",
    "    Dense(1, activation=\"sigmoid\")])\n",
    "\n",
    "l2_model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "l2_model_hist = l2_model.fit(\n",
    "    train_data, train_labels,\n",
    "    epochs=20, batch_size=512, validation_split=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0413ea",
   "metadata": {},
   "source": [
    "$L^2$-규제된 신경망은 학습데이터의 손실함수값이 일정 수준 이하로는 내려오질 않네요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f99d808",
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_model_loss = l2_model_hist.history['loss']\n",
    "\n",
    "plt.plot(epochs, original_train_loss, 'b+', label='Original model')\n",
    "plt.plot(epochs, l2_model_loss, 'bo', label='L2-regularized model')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Training loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ee2ce9",
   "metadata": {},
   "source": [
    "$L^2$-규제된 신경망은 과적합이 억제되고 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6358355c",
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_model_val_loss = l2_model_hist.history['val_loss']\n",
    "\n",
    "plt.plot(epochs, original_val_loss, 'b+', label='Original model')\n",
    "plt.plot(epochs, l2_model_val_loss, 'bo', label='L2-regularized model')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739e593e",
   "metadata": {},
   "source": [
    "**[실습1] (10분) 가중치 감소 계수 $\\lambda$를 약하게 $0.0002$로 잡은 모델과 강하게 $0.02$로 잡은 모델을 학습시키고 검증데이터의 손실함수 히스토리를 그래프로 그리시오.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e142c2b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b5456737",
   "metadata": {},
   "source": [
    "# $L^1$-규제 ($L^1$-regularization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfebebcf",
   "metadata": {},
   "source": [
    "우리는 $L^1$-규제에 대해 공부했습니다.  \n",
    "기존 손실함수에 $L^1$ 페널티 ${1 \\over 2} \\times \\lambda \\times \\sum |w_{i,j}|$를 더합니다.  \n",
    "여기서 $w_{i,j}$는 모든 가중치를 뜻합니다.  \n",
    "가중치 감소 계수 $\\lambda$는 직접 설정해야 하는 하이퍼 파라미터입니다.  \n",
    "크게 잡으면 페널티가 큰 역할을 하고 작게 잡으면 페널티 역할이 작아집니다.  \n",
    "미분은 선형이기 때문에 새로운 손실함수의 그레디언트는 기존 손실함수의 그레디언트와 페널티의 그레디언트를 합한 것과 같습니다.  \n",
    "경사하강법을 적용하면 각 그레디언트의 반대방향의 합으로 가중치를 업데이트합니다.  \n",
    "각각은 정반대 역할을 합니다.  \n",
    "기존 손실함수의 그레디언트의 반대방향으로 업데이트하면 신경망은 훈련데이터의 정보를 더 담아냅니다.  \n",
    "$L^1$-페널티의 그레디언트의 반대방향 업데이트는 절대값이 작은 가중치를 아예 없애버리려는 경향이 있습니다.  \n",
    "이는 습득한 정보를 뭉게버리는 효과가 있습니다.  \n",
    "케라스에서는 [tf.keras.regularizers.L1](https://www.tensorflow.org/api_docs/python/tf/keras/regularizers/L1)를 통해 구현되어 있습니다.  \n",
    "[Dense](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense) 클래스에서 인수로 설정할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9affbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_model = keras.Sequential([\n",
    "    Dense(16, kernel_regularizer=regularizers.l1(0.001), activation=\"relu\"),\n",
    "    Dense(16, kernel_regularizer=regularizers.l1(0.001), activation=\"relu\"),\n",
    "    Dense(1, activation=\"sigmoid\")])\n",
    "\n",
    "l1_model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "l1_model_hist = l1_model.fit(\n",
    "    train_data, train_labels,\n",
    "    epochs=20, batch_size=512, validation_split=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0479f31e",
   "metadata": {},
   "source": [
    "**[실습2] (5분) 기본 신경망과 $L^1$-규제된 신경망의 훈련데이터에 대한 손실함수값 히스토리를 그래프로 그리시오.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ef9496",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "879f96de",
   "metadata": {},
   "source": [
    "**[실습3] (5분) 기본 신경망과 $L^1$-규제된 신경망의 검증데이터에 대한 손실함수값 히스토리를 그래프로 그리시오.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5e6c2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "35037841",
   "metadata": {},
   "source": [
    "# Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540e82c1",
   "metadata": {},
   "source": [
    "이전에 공부한 dropout을 케라스로 구현하겠습니다.  \n",
    "dropout은 학습 데이터가 들어올때마다 무작위로 뉴런을 삭제해서 신호전달을 차단하는 기법입니다.  \n",
    "이상해보이지만 과적합 억제를 위해 매우 많이 사용되는 기법입니다.  \n",
    "케라스에는 [Dropout층](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dropout)이 구현되어 있습니다.  \n",
    "몇 퍼센트를 삭제할지 인수로 지정해 줍니다.  \n",
    "![](https://drive.google.com/thumbnail?id=1wPtyMrgWXGxiJp0YofJPGlYDDE94uoX-&sz=s4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b8c505",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import plot_model\n",
    "from keras.layers import Dropout\n",
    "\n",
    "dropout_model = keras.Sequential([\n",
    "    Dense(16, input_shape=(10000,), activation=\"relu\"),\n",
    "    Dropout(0.5),\n",
    "    Dense(16, activation=\"relu\"),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation=\"sigmoid\")])\n",
    "\n",
    "dropout_model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "dropout_model_hist = dropout_model.fit(\n",
    "    train_data, train_labels,\n",
    "    epochs=20, batch_size=512, validation_split=0.4)\n",
    "\n",
    "plot_model(dropout_model, show_shapes=True, show_layer_activations=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f753bc9",
   "metadata": {},
   "source": [
    "dropout층이 추가된 신경망은 학습데이터의 손실함수값이 천천히 떨어지네요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf6762d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_model_loss = dropout_model_hist.history['loss']\n",
    "\n",
    "plt.plot(epochs, original_train_loss, 'b+', label='Original model')\n",
    "plt.plot(epochs, dropout_model_loss, 'bo', label='Dropout-regularized model')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Training loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7997b56",
   "metadata": {},
   "source": [
    "dropout층이 추가된 신경망은 과적합이 늦게 시작되고 더 완만하게 진행됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f7b3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_model_val_loss = dropout_model_hist.history['val_loss']\n",
    "\n",
    "plt.plot(epochs, original_val_loss, 'b+', label='Original model')\n",
    "plt.plot(epochs, dropout_model_val_loss, 'bo', label='Dropout-regularized model')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9778cb",
   "metadata": {},
   "source": [
    "**[실습4] (10분) dropout 비율을 약하게 $0.2$로 잡은 모델과 강하게 $0.8$로 잡은 모델을 학습시키고 검증데이터의 손실함수 히스토리를 그래프로 그리시오.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b8820b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
