{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca936fe8",
   "metadata": {
    "id": "ca936fe8"
   },
   "source": [
    "케라스의 [tf.keras.datasets.boston_housing](https://www.tensorflow.org/api_docs/python/tf/keras/datasets/boston_housing)로부터 보스턴 주택가격 데이터셋을 불러오겠습니다.\n",
    "- 1970년대 보스턴 교외의 타운별 데이터 506개\n",
    "- 각 행은 타운별 데이터\n",
    "- 각 열은 주택 가격에 영향을 미치는 요소 13개\n",
    "- 훈련용 404개, 테스트용 102개\n",
    "- 라벨은 주택가격 중앙값 (단위 : 천달러)  \n",
    "![](https://drive.google.com/thumbnail?id=1GkxuiFIvsL_k0u5iYNmWsF6oaOG3Ed4P&sz=s4000)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1897c27d",
   "metadata": {
    "id": "1897c27d"
   },
   "source": [
    "# 보스턴 주택가격 데이터셋"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1ce743",
   "metadata": {
    "id": "ba1ce743"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from keras.datasets import boston_housing\n",
    "(train_data, train_targets), (test_data, test_targets) = boston_housing.load_data()\n",
    "\n",
    "print(f\"훈련용 : {train_data.shape}\")\n",
    "print(f\"테스트용 : {test_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17a4659",
   "metadata": {
    "id": "b17a4659"
   },
   "source": [
    "주택 가격에 영향을 미치는 13개의 요소는 다음과 같습니다.\n",
    "- CRIM : per capita crime rate by town (범죄율)\n",
    "- ZN : proportion of residential land zoned for lots over 25,000 sq.ft.\n",
    "- INDUS : proportion of non-retail business acres per town.\n",
    "- CHAS : Charles River dummy variable (1 if tract bounds river; 0 otherwise) (강변)\n",
    "- NOX : nitric oxides concentration (parts per 10 million)\n",
    "- RM : average number of rooms per dwelling (평균 방 개수)\n",
    "- AGE : proportion of owner-occupied units built prior to 1940 (노후주택 비율)\n",
    "- DIS : weighted distances to five Boston employment centres\n",
    "- RAD : index of accessibility to radial highways\n",
    "- TAX : full-value property-tax rate per $10,000 (재산세 세율)\n",
    "- PTRATIO : pupil-teacher ratio by town (학생/교사 비율)\n",
    "- B : 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
    "- LSTAT : lower status of the population (하위계층 비율)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83b6fdc",
   "metadata": {
    "id": "b83b6fdc"
   },
   "outputs": [],
   "source": [
    "columns = ['CRIM','ZN','INDUS','CHAS','NOX','RM','AGE','DIS','RAD','TAX','PTRATIO','B','LSTAT']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f41c94c",
   "metadata": {
    "id": "0f41c94c"
   },
   "source": [
    "파이썬데이터분석 시간에 배운 판다스를 이용해 훈련 데이터를 출력해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b826295a",
   "metadata": {
    "id": "b826295a"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(train_data, columns=columns)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339c0e4b",
   "metadata": {
    "id": "339c0e4b"
   },
   "source": [
    "라벨을 마지막 열에 추가하겠습니다.  \n",
    "타운별 주택가격 중앙값입니다.  \n",
    "단위는 천달러입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3565df",
   "metadata": {
    "id": "7f3565df"
   },
   "outputs": [],
   "source": [
    "df['PRICE'] = train_targets\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bcad556",
   "metadata": {
    "id": "6bcad556"
   },
   "source": [
    "기본적인 통계정보를 출력해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5749b6a1",
   "metadata": {
    "id": "5749b6a1"
   },
   "outputs": [],
   "source": [
    "train_stats = df.describe()\n",
    "train_stats.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d81437",
   "metadata": {
    "id": "32d81437"
   },
   "source": [
    "seaborn의 [pairplot](https://seaborn.pydata.org/generated/seaborn.pairplot.html)을 사용해서 몇몇 조합의의 산포도를 그려보겠습니다.  \n",
    "주택가격 예측이 목적이므로 맨 오른쪽 열을 순서대로 해석해보죠.\n",
    "- 대부분 범죄율(CRIM)이 낮지만 왼쪽 위에 점이 몇개 찍혀 있습니다. 우범지대겠지요. 집값(PRICE)이 낮습니다.\n",
    "- 방 개수(RM)는 집값(PRICE)과 비례합니다.\n",
    "- 하위계층비율(LSTAT)은 집값과 반비례합니다.\n",
    "- 집값(PRICE)의 히스토그램입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63165734",
   "metadata": {
    "id": "63165734"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.pairplot(df[[\"CRIM\",\"RM\",\"LSTAT\",\"PRICE\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5b7d46",
   "metadata": {
    "id": "6c5b7d46"
   },
   "source": [
    "**[실습1] (5분) 테스트 데이터에 라벨을 추가한 데이터 프레임을 출력하시오.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc6b37d",
   "metadata": {
    "id": "2dc6b37d"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d5af83d0",
   "metadata": {
    "id": "d5af83d0"
   },
   "source": [
    "각 특성들은 스케일이 매우 다릅니다.  \n",
    "학습시 스케일이 작은 특성은 무시되고 스케일이 큰 특성은 지배적이 됩니다.  \n",
    "결과적으로는 적은 정보를 지닌 데이터로 학습시키는 셈이 됩니다.  \n",
    "이를 방지하기 위해 데이터를 특성별로 정규화해야합니다.  \n",
    "주의할 점은 테스트 데이터를 정규화할 때 훈련 데이터의 평균과 표준편차를 사용한다는 점입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52898061",
   "metadata": {
    "id": "52898061"
   },
   "outputs": [],
   "source": [
    "mean = train_data.mean(axis=0)\n",
    "train_data -= mean\n",
    "std = train_data.std(axis=0)\n",
    "train_data /= std\n",
    "test_data -= mean\n",
    "test_data /= std"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc89b9ab",
   "metadata": {
    "id": "fc89b9ab"
   },
   "source": [
    "# 회귀"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42e82aa",
   "metadata": {
    "id": "b42e82aa"
   },
   "source": [
    "우리가 여태까지 공부한 문제는 분류(classification)문제입니다.  \n",
    "시험으로 치면 객관식 문제를 푸는 셈이지요.  \n",
    "MNIST, Fashion MNIST는 10지선다 문제를 푸는 셈이고 로이터 뉴스는 46지선다 문제를 푸는 셈이고 IMDB는 OX 문제를 푸는 셈입니다.  \n",
    "분류문제에서는 통상적으로 교차 엔트로피(cross entropy)를 손실함수로 사용합니다. 또한, 평가지표로는 정확도(accuracy)를 사용합니다.  \n",
    "![](https://drive.google.com/thumbnail?id=1wdEkIUaKqrAh76qYB41eKUkEKHergDy0&sz=s4000)\n",
    "\n",
    "---\n",
    "회귀(regression)문제는 시험으로 치면 주관식 문제를 푸는 셈입니다.  \n",
    "내일 더울지 추울지 예측하는 문제는 분류문제이고 내일 기온을 예측하는 문제는 회귀문제입니다.  \n",
    "회귀문제에서는 평가지표로 정확도를 사용해서는 안됩니다.  \n",
    "내일 기온을 칼같이 예측하기는 어려울테니까요.  \n",
    "회귀문제에서는 평가지표로 평균절대오차(Mean Absolute Error)를 사용합니다.  \n",
    "두 벡터 $(x_1,x_2,\\cdots,x_n)$, $(y_1,y_2,\\cdots,y_n)$의 평균절대오차는 이름 그대로 오차의 절대값의 평균 ${\\rm MAE} = {1 \\over n}\\sum_{i=1}^n |y_i-x_i|$으로 정의합니다.  \n",
    "평가시 평균절대오차는 작을수록 좋습니다 (정확도는 클수록 좋았죠).  \n",
    "손실함수는 통상적으로 평균제곱오차(Mean Squared Error)를 사용합니다.  \n",
    "두 벡터 $(x_1,x_2,\\cdots,x_n)$, $(y_1,y_2,\\cdots,y_n)$의 평균제곱오차는 이름 그대로 오차의 제곱의 평균 ${\\rm MSE} = {1 \\over n}\\sum_{i=1}^n (y_i-x_i)^2$으로 정의합니다.  \n",
    "평균제곱오차는 [딥러닝I](https://youtu.be/Bj4TS9ip9SM?t=897)에서 등장은 했는데 실제로는 교차엔트로피만 사용해왔죠.  \n",
    "![](https://drive.google.com/thumbnail?id=1kDAhPbUz7_uNGF_sc1HEGcB0EzAd87ik&sz=s4000)\n",
    "\n",
    "---\n",
    "예를 들어, $(1,2,3,4,5)$와 $(3,3,3,3,3)$의 평균절대오차는\n",
    "$$\n",
    "{\\rm MAE} = {1 \\over 5}(|1-3|+|2-3|+|3-3|+|4-3|+|5-3|) = {1 \\over 5}(2+1+0+1+2) = {6 \\over 5}\n",
    "$$\n",
    "이고 평균제곱오차는\n",
    "$$\n",
    "{\\rm MSE} = {1 \\over 5}((1-3)^2+(2-3)^2+(3-3)^2+(4-3)^2+(5-3)^2) = {1 \\over 5}(4+1+0+1+4) = 2\n",
    "$$\n",
    "입니다.  \n",
    "이 예에서 보다시피 분산이 평균분포와의 평균제곱오차입니다.\n",
    "\n",
    "---\n",
    "신경망을 다음과 같이 구성하겠습니다.  \n",
    "입력 뉴런의 수는 데이터의 특성수이고 출력 뉴런의 수는 1입니다.  \n",
    "이진분류에서는 마지막층에서 시그모이드 변환을 했지만 회귀에서는 통상적으로 아무것도 하지 않습니다.  \n",
    "이진분류에서는 확률을 출력해야 하니까 시그모이드 변환을 했지만 주택가격은 0가 1사이에 있을 필요가 없으니까요.  \n",
    "[compile](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential#compile) 메서드에서 `loss=[\"mse\"]`로 손실함수를 평균제곱오차로 설정하고 `metrics=\"mae\"`로 평가지표를 평균절대오차로 설정합니다.  \n",
    "앞으로 신경망을 반복해서 초기화할거라서 아예 신경망을 생성하는 함수를 만들어 놓겠습니다.  \n",
    "![](https://drive.google.com/thumbnail?id=1di1n97y6s1Fbsr-B-9wl-m1onhdlt_w6&sz=s4000)\n",
    "\n",
    "---\n",
    "신경망의 출력값이 $x_1$이고 라벨이 $y_1$이라면 평균제곱오차는 $(x_1-y_1)^2$이고 평균절대오차는 $|x_1-y_2|$입니다.  \n",
    "$n$개의 배치묶음이 입력되어 출력값이 $x_1,x_2,\\cdots,x_n$이고 라벨이 $y_1,\\cdots,y_n$이라면 평균을 하게 되어 ${1 \\over n}\\sum_{i=1}^n (y_i-x_i)^2$와 ${1 \\over n}\\sum_{i=1}^n |y_i-x_i|$이 나옵니다.  \n",
    "다시말해 배치묶음을 입력하면 두 벡터 $(x_1,x_2,\\cdots,x_n)$, $(y_1,y_2,\\cdots,y_n)$의 평균제곱오차와 평균절대오차가 됩니다.\n",
    "\n",
    "---\n",
    "입력데이터는 파란색입니다.  \n",
    "이를 입력받아서 역전파를 통해 가급적 빨간색과 비슷한 값을 신경망이 출력할 수 있도록 파라미터들을 업데이트합니다.  \n",
    "![](https://drive.google.com/thumbnail?id=1VytfDs5vZowI7e6GjRX40ZPPqrMvHTg4&sz=s4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31157f82",
   "metadata": {
    "id": "31157f82"
   },
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras.layers import Dense\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "def build_model():\n",
    "    model = keras.Sequential([\n",
    "        Dense(64, input_shape=(13,), activation=\"relu\"),\n",
    "        Dense(64, activation=\"relu\"),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer=\"rmsprop\", loss=\"mse\", metrics=[\"mae\"])\n",
    "    return model\n",
    "\n",
    "model=build_model()\n",
    "plot_model(model, show_shapes=True, show_layer_activations=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e71034",
   "metadata": {
    "id": "31e71034"
   },
   "source": [
    "# K-겹 검증 (K-fold Validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe76f40e",
   "metadata": {
    "id": "fe76f40e"
   },
   "source": [
    "훈련 데이터가 404개 밖에 없습니다.  \n",
    "하이퍼 파라미터 탐색을 위해 훈련데이터와 검증데이터로 분할해야 하는데 데이터가 적어서 검증데이터에 적은 양을 할당할 수 밖에 없습니다.  \n",
    "적은 양때문에 샘플링 편차가 커서 검증 결과를 믿을 수가 없습니다.  \n",
    "이럴때 쓰는 방법이 K-겹 검증입니다.  \n",
    "훈련 데이터를 K-등분한 후 하나를 검증데이터로 사용합니다.  \n",
    "이렇게 얻은 K개의 검증결과를 평균합니다.  \n",
    "\n",
    "![](https://drive.google.com/thumbnail?id=1N9g6DhhKpj0WjzzIj1ZDRN7YmZAj8UVm&sz=s4000)\n",
    "\n",
    "---\n",
    "4겹 검증 코드입니다.  \n",
    "훈련 데이터의 개수가 404개이므로 `num_val_samples`는 101입니다.  \n",
    "훈련데이터를 4등분해서 그중에 i번째 블럭(그림에서 파란 부분)을 뽑아내서 `val_data`라고 둡니다.  \n",
    "i번째 블럭의 왼쪽 블럭과 오른쪽 블럭(그림에서 녹색부분)을 `np.concatenate`를 이용해 합쳐서 `partial_train_data`를 만듭니다.  \n",
    "행렬 2개를 합치는 것이므로 옆으로 붙일지 위아래로 붙일지를 명시해야 합니다.  \n",
    "위아래로 붙여야 하므로 `axis=0`으로 설정합니다.  \n",
    "위에서 정의한 함수로 모델을 만든후 `partial_train_data`로 훈련시킵니다.  \n",
    "`val_data`로 성능을 평가해 평균절대오차를 `all_scores`에 추가합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9096a54a",
   "metadata": {
    "id": "9096a54a"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "k = 4\n",
    "num_val_samples = len(train_data) // k\n",
    "num_epochs = 100\n",
    "all_scores = []\n",
    "for i in range(k):\n",
    "    print(f\"Processing fold #{i}\")\n",
    "    val_data = train_data[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "    val_targets = train_targets[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "    partial_train_data = np.concatenate(\n",
    "        [train_data[:i * num_val_samples],\n",
    "         train_data[(i + 1) * num_val_samples:]],\n",
    "        axis=0)\n",
    "    partial_train_targets = np.concatenate(\n",
    "        [train_targets[:i * num_val_samples],\n",
    "         train_targets[(i + 1) * num_val_samples:]])\n",
    "    model = build_model()\n",
    "    model.fit(partial_train_data, partial_train_targets,\n",
    "              epochs=num_epochs, batch_size=16, verbose=0)\n",
    "    val_mse, val_mae = model.evaluate(val_data, val_targets, verbose=0)\n",
    "    all_scores.append(val_mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3676a6",
   "metadata": {
    "id": "1e3676a6"
   },
   "source": [
    "샘플링 수가 적어서 편차가 꽤 되네요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1703a1",
   "metadata": {
    "id": "2a1703a1"
   },
   "outputs": [],
   "source": [
    "print(all_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f9a42d",
   "metadata": {
    "id": "14f9a42d"
   },
   "source": [
    "넷을 평균하면 더 신뢰할 수 있습니다.  \n",
    "K-겹 검증을 하는 이유입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c668c07",
   "metadata": {
    "id": "6c668c07"
   },
   "outputs": [],
   "source": [
    "print(np.mean(all_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e213c6d",
   "metadata": {
    "id": "3e213c6d"
   },
   "source": [
    "본격적으로 K-겹 검증을 사용해서 최적의 학습회수를 찾겠습니다.  \n",
    "오랫동안 훈련시켜서 과적합을 일으킵니다.  \n",
    "한참 걸리니 잠시 쉬었다 오세요.  \n",
    "`mae_history`는 에퍽이 끝날때마다 검증데이터로 측정한 평균절대오차들을 모아놓은 리스트\n",
    "$$\n",
    "[a_1, a_2, \\cdots, a_{500}]\n",
    "$$\n",
    "입니다.  \n",
    "모델이 4개이므로 `mae_history`도 4개가 나올텐데 `all_mae_histories`는 이들로 이루어진 리스트\n",
    "$$\n",
    "[[a_1, a_2, \\cdots, a_{500}], [b_1, b_2, \\cdots, b_{500}], [c_1, c_2, \\cdots, c_{500}], [d_1, d_2, \\cdots, d_{500}]]\n",
    "$$\n",
    "입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28e4d8c",
   "metadata": {
    "id": "f28e4d8c"
   },
   "outputs": [],
   "source": [
    "num_epochs = 500\n",
    "all_mae_histories = []\n",
    "for i in range(k):\n",
    "    print(f\"Processing fold #{i}\")\n",
    "    val_data = train_data[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "    val_targets = train_targets[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "    partial_train_data = np.concatenate(\n",
    "        [train_data[:i * num_val_samples],\n",
    "         train_data[(i + 1) * num_val_samples:]],\n",
    "        axis=0)\n",
    "    partial_train_targets = np.concatenate(\n",
    "        [train_targets[:i * num_val_samples],\n",
    "         train_targets[(i + 1) * num_val_samples:]])\n",
    "    model = build_model()\n",
    "    history = model.fit(partial_train_data, partial_train_targets,\n",
    "                        validation_data=(val_data, val_targets),\n",
    "                        epochs=num_epochs, batch_size=16, verbose=0)\n",
    "    mae_history = history.history[\"val_mae\"]\n",
    "    all_mae_histories.append(mae_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a0061b",
   "metadata": {
    "id": "e0a0061b"
   },
   "source": [
    "각 에퍽별로 4개의 평균절대오차를 평균합니다.  \n",
    "`average_mae_history`는 에퍽이 끝날때마다 측정한 4개 모델의 평균절대오차들의 평균을 모아놓은 리스트\n",
    "$$\n",
    "[{a_1+b_1+c_1+d_1 \\over 4}, {a_2+b_2+c_2+d_2 \\over 4}, \\cdots, {a_{500}+b_{500}+c_{500}+d_{500} \\over 4}]\n",
    "$$\n",
    "입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967d58ce",
   "metadata": {
    "id": "967d58ce"
   },
   "outputs": [],
   "source": [
    "average_mae_history = [np.mean([x[i] for x in all_mae_histories]) for i in range(num_epochs)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc4e9f3",
   "metadata": {
    "id": "6fc4e9f3"
   },
   "source": [
    "그래프를 그려보면 초반 평균절대오차가 너무 커서 언제부터 과적합이 일어나는지 눈으로 확인이 어렵습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb73f011",
   "metadata": {
    "id": "cb73f011"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(range(1, len(average_mae_history) + 1), average_mae_history)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Validation MAE\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b284e0c",
   "metadata": {
    "id": "9b284e0c"
   },
   "source": [
    "지나치게 큰 초반 10에퍽은 제외하겠습니다.  \n",
    "변동이 크지만 대략 130에퍽부터 과적합이 시작되네요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c01ecf",
   "metadata": {
    "id": "b5c01ecf"
   },
   "outputs": [],
   "source": [
    "truncated_mae_history = average_mae_history[10:]\n",
    "plt.plot(range(1, len(truncated_mae_history) + 1), truncated_mae_history)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Validation MAE\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc964251",
   "metadata": {
    "id": "fc964251"
   },
   "source": [
    "# 학습, 평가, 예측"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37dfcfa",
   "metadata": {
    "id": "a37dfcfa"
   },
   "source": [
    "훈련 데이터 전체를 사용해 130에퍽동안 훈련시키겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c0d410",
   "metadata": {
    "id": "47c0d410"
   },
   "outputs": [],
   "source": [
    "model = build_model()\n",
    "model.fit(train_data, train_targets,\n",
    "          epochs=130, batch_size=16, verbose=0)\n",
    "test_mse_score, test_mae_score = model.evaluate(test_data, test_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8db07f9",
   "metadata": {
    "id": "c8db07f9"
   },
   "source": [
    "학습할때마다 달라지지만 테스트 데이터로 확인한 최종 평균절대오차는 대략 2,600달러 정도네요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff12027f",
   "metadata": {
    "id": "ff12027f"
   },
   "outputs": [],
   "source": [
    "test_mae_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e9ce5e",
   "metadata": {
    "id": "b3e9ce5e"
   },
   "source": [
    "첫번째 테스트 데이터에 대한 예측가격과 실제가격입니다.  \n",
    "학습할때마다 오차는 달라집니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecef5d64",
   "metadata": {
    "id": "ecef5d64"
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(test_data)\n",
    "print(f\"예측가격 : {predictions[0]}\")\n",
    "print(f\"실제가격 : {test_targets[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f32b06",
   "metadata": {
    "id": "a0f32b06"
   },
   "source": [
    "**[실습2] (5분) 1번 문제에서 구한 데이터프레임에 신경망의 예측값을 추가해서 출력하시오.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f20c956",
   "metadata": {
    "id": "6f20c956"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9d8c9d74",
   "metadata": {
    "id": "9d8c9d74"
   },
   "source": [
    "**[실습3] (5분) 실제가격과 예측가격의 산포도를 그리시오. $y=x$의 그래프도 그려넣으시오.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce022953",
   "metadata": {
    "id": "ce022953"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "28b0771e",
   "metadata": {
    "id": "28b0771e"
   },
   "source": [
    "**[실습4] (5분) (예측가격 - 실제가격)의 히스토그램을 그리시오.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b637b126",
   "metadata": {
    "id": "b637b126"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "462d9c65",
   "metadata": {
    "id": "462d9c65"
   },
   "source": [
    "**[실습5] (10분) (i) 앞 8개 특성만 사용해서 훈련시킨후 평가하시오.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4584febe",
   "metadata": {
    "id": "4584febe"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ef7bca5d",
   "metadata": {
    "id": "ef7bca5d"
   },
   "source": [
    "**(ii) 앞 4개 특성만 사용해서 훈련시킨후 평가하시오.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25e51d2",
   "metadata": {
    "id": "e25e51d2"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ef2f273f",
   "metadata": {
    "id": "ef2f273f"
   },
   "source": [
    "**[과제1] 표준정규분포를 따라 랜덤하게 k개의 노이즈 특성을 만들어 훈련데이터에 추가하시오. 표준정규분포를 따라 랜덤하게 k개의 노이즈 특성을 만들어 테스트 데이터에 추가하시오. 13+k의 특성으로 신경망을 학습시킨후 테스트 데이터로 평균절대오차를 출력하시오. (k=5,10,20)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70c6c52",
   "metadata": {
    "id": "d70c6c52"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "37476bd2",
   "metadata": {
    "id": "37476bd2"
   },
   "source": [
    "# 싸이킷런을 이용한 K-겹 검증"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdae1db5",
   "metadata": {
    "id": "bdae1db5"
   },
   "source": [
    "[sklearn.model_selection.KFold](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html)를 이용하면 더 편하게 K-겹 검증 코드를 작성할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3cbef69",
   "metadata": {
    "id": "d3cbef69"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "k = 4\n",
    "num_epochs = 100\n",
    "all_scores = []\n",
    "kfold = KFold(n_splits=k)\n",
    "\n",
    "for train_idx, val_idx in kfold.split(train_data):\n",
    "    partial_train_data, val_data = train_data[train_idx], train_data[val_idx]\n",
    "    partial_train_targets, val_targets = train_targets[train_idx], train_targets[val_idx]\n",
    "    model = build_model()\n",
    "    model.fit(partial_train_data, partial_train_targets,\n",
    "              epochs=num_epochs, batch_size=16, verbose=0)\n",
    "    val_mse, val_mae = model.evaluate(val_data, val_targets, verbose=0)\n",
    "    all_scores.append(val_mae)\n",
    "\n",
    "print(all_scores)\n",
    "print(np.mean(all_scores))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
