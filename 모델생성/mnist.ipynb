{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.5.1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn\n",
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "\n",
    "mnist = load_digits()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DESCR', 'data', 'feature_names', 'frame', 'images', 'target', 'target_names']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, ..., 8, 9, 8])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(mnist.target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f42ebdf99d0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(mnist.images[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset # 텐서데이터셋\n",
    "from torch.utils.data import DataLoader # 데이터로더\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.FloatTensor(mnist.data)\n",
    "y = torch.LongTensor(mnist.target)\n",
    "x_train , x_test , y_train ,y_test = train_test_split(x , y , test_size=0.2, random_state=11)\n",
    "\n",
    "dataset = TensorDataset(x_train,y_train)\n",
    "\n",
    "dataloader = DataLoader(dataset,batch_size=32,shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class fun1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(64,32)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(32,10)\n",
    "        # self.softmax = nn.Softmax(dim = 10)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.linear(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        # x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "model = fun1()\n",
    "        \n",
    "\n",
    "optimizer = optim.Adam(model.parameters() , lr= 0.005)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "epochs = 15\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0 Cost: 3.426354\n",
      "Epoch    0 Cost: 2.715164\n",
      "Epoch    0 Cost: 2.400490\n",
      "Epoch    0 Cost: 2.106102\n",
      "Epoch    0 Cost: 1.832762\n",
      "Epoch    0 Cost: 1.618662\n",
      "Epoch    0 Cost: 1.455839\n",
      "Epoch    0 Cost: 1.324276\n",
      "Epoch    0 Cost: 1.194434\n",
      "Epoch    0 Cost: 1.055030\n",
      "Epoch    0 Cost: 0.921901\n",
      "Epoch    0 Cost: 0.812360\n",
      "Epoch    0 Cost: 0.724352\n",
      "Epoch    0 Cost: 0.646562\n",
      "Epoch    0 Cost: 0.574000\n",
      "Epoch    0 Cost: 0.508112\n",
      "Epoch    0 Cost: 0.452804\n",
      "Epoch    0 Cost: 0.410696\n",
      "Epoch    0 Cost: 0.379542\n",
      "Epoch    0 Cost: 0.352733\n",
      "Epoch    0 Cost: 0.327075\n",
      "Epoch    0 Cost: 0.304042\n",
      "Epoch    0 Cost: 0.284870\n",
      "Epoch    0 Cost: 0.268682\n",
      "Epoch    0 Cost: 0.253366\n",
      "Epoch    0 Cost: 0.238364\n",
      "Epoch    0 Cost: 0.225549\n",
      "Epoch    0 Cost: 0.215315\n",
      "Epoch    0 Cost: 0.205715\n",
      "Epoch    0 Cost: 0.195479\n",
      "Epoch    0 Cost: 0.185819\n",
      "Epoch    0 Cost: 0.178137\n",
      "Epoch    0 Cost: 0.171777\n",
      "Epoch    0 Cost: 0.165414\n",
      "Epoch    0 Cost: 0.158654\n",
      "Epoch    0 Cost: 0.152241\n",
      "Epoch    0 Cost: 0.146874\n",
      "Epoch    0 Cost: 0.142171\n",
      "Epoch    0 Cost: 0.137363\n",
      "Epoch    0 Cost: 0.132286\n",
      "Epoch    0 Cost: 0.127418\n",
      "Epoch    0 Cost: 0.123130\n",
      "Epoch    0 Cost: 0.119318\n",
      "Epoch    0 Cost: 0.115647\n",
      "Epoch    0 Cost: 0.111996\n",
      "Epoch    1 Cost: 0.108578\n",
      "Epoch    1 Cost: 0.105495\n",
      "Epoch    1 Cost: 0.102649\n",
      "Epoch    1 Cost: 0.099889\n",
      "Epoch    1 Cost: 0.097191\n",
      "Epoch    1 Cost: 0.094581\n",
      "Epoch    1 Cost: 0.092106\n",
      "Epoch    1 Cost: 0.089755\n",
      "Epoch    1 Cost: 0.087478\n",
      "Epoch    1 Cost: 0.085239\n",
      "Epoch    1 Cost: 0.083066\n",
      "Epoch    1 Cost: 0.081018\n",
      "Epoch    1 Cost: 0.079078\n",
      "Epoch    1 Cost: 0.077209\n",
      "Epoch    1 Cost: 0.075411\n",
      "Epoch    1 Cost: 0.073717\n",
      "Epoch    1 Cost: 0.072141\n",
      "Epoch    1 Cost: 0.070636\n",
      "Epoch    1 Cost: 0.069144\n",
      "Epoch    1 Cost: 0.067685\n",
      "Epoch    1 Cost: 0.066325\n",
      "Epoch    1 Cost: 0.065034\n",
      "Epoch    1 Cost: 0.063749\n",
      "Epoch    1 Cost: 0.062457\n",
      "Epoch    1 Cost: 0.061201\n",
      "Epoch    1 Cost: 0.060008\n",
      "Epoch    1 Cost: 0.058863\n",
      "Epoch    1 Cost: 0.057742\n",
      "Epoch    1 Cost: 0.056642\n",
      "Epoch    1 Cost: 0.055568\n",
      "Epoch    1 Cost: 0.054530\n",
      "Epoch    1 Cost: 0.053523\n",
      "Epoch    1 Cost: 0.052538\n",
      "Epoch    1 Cost: 0.051580\n",
      "Epoch    1 Cost: 0.050650\n",
      "Epoch    1 Cost: 0.049747\n",
      "Epoch    1 Cost: 0.048873\n",
      "Epoch    1 Cost: 0.048026\n",
      "Epoch    1 Cost: 0.047197\n",
      "Epoch    1 Cost: 0.046388\n",
      "Epoch    1 Cost: 0.045604\n",
      "Epoch    1 Cost: 0.044839\n",
      "Epoch    1 Cost: 0.044087\n",
      "Epoch    1 Cost: 0.043349\n",
      "Epoch    1 Cost: 0.042626\n",
      "Epoch    2 Cost: 0.041917\n",
      "Epoch    2 Cost: 0.041222\n",
      "Epoch    2 Cost: 0.040540\n",
      "Epoch    2 Cost: 0.039873\n",
      "Epoch    2 Cost: 0.039219\n",
      "Epoch    2 Cost: 0.038579\n",
      "Epoch    2 Cost: 0.037955\n",
      "Epoch    2 Cost: 0.037343\n",
      "Epoch    2 Cost: 0.036742\n",
      "Epoch    2 Cost: 0.036149\n",
      "Epoch    2 Cost: 0.035565\n",
      "Epoch    2 Cost: 0.034991\n",
      "Epoch    2 Cost: 0.034429\n",
      "Epoch    2 Cost: 0.033876\n",
      "Epoch    2 Cost: 0.033332\n",
      "Epoch    2 Cost: 0.032798\n",
      "Epoch    2 Cost: 0.032274\n",
      "Epoch    2 Cost: 0.031762\n",
      "Epoch    2 Cost: 0.031259\n",
      "Epoch    2 Cost: 0.030768\n",
      "Epoch    2 Cost: 0.030286\n",
      "Epoch    2 Cost: 0.029822\n",
      "Epoch    2 Cost: 0.029364\n",
      "Epoch    2 Cost: 0.028912\n",
      "Epoch    2 Cost: 0.028469\n",
      "Epoch    2 Cost: 0.028038\n",
      "Epoch    2 Cost: 0.027616\n",
      "Epoch    2 Cost: 0.027196\n",
      "Epoch    2 Cost: 0.026787\n",
      "Epoch    2 Cost: 0.026392\n",
      "Epoch    2 Cost: 0.026004\n",
      "Epoch    2 Cost: 0.025618\n",
      "Epoch    2 Cost: 0.025243\n",
      "Epoch    2 Cost: 0.024874\n",
      "Epoch    2 Cost: 0.024509\n",
      "Epoch    2 Cost: 0.024150\n",
      "Epoch    2 Cost: 0.023800\n",
      "Epoch    2 Cost: 0.023459\n",
      "Epoch    2 Cost: 0.023125\n",
      "Epoch    2 Cost: 0.022801\n",
      "Epoch    2 Cost: 0.022482\n",
      "Epoch    2 Cost: 0.022168\n",
      "Epoch    2 Cost: 0.021861\n",
      "Epoch    2 Cost: 0.021556\n",
      "Epoch    2 Cost: 0.021256\n",
      "Epoch    3 Cost: 0.020965\n",
      "Epoch    3 Cost: 0.020677\n",
      "Epoch    3 Cost: 0.020396\n",
      "Epoch    3 Cost: 0.020122\n",
      "Epoch    3 Cost: 0.019853\n",
      "Epoch    3 Cost: 0.019587\n",
      "Epoch    3 Cost: 0.019327\n",
      "Epoch    3 Cost: 0.019075\n",
      "Epoch    3 Cost: 0.018820\n",
      "Epoch    3 Cost: 0.018571\n",
      "Epoch    3 Cost: 0.018340\n",
      "Epoch    3 Cost: 0.018099\n",
      "Epoch    3 Cost: 0.017869\n",
      "Epoch    3 Cost: 0.017641\n",
      "Epoch    3 Cost: 0.017418\n",
      "Epoch    3 Cost: 0.017207\n",
      "Epoch    3 Cost: 0.016985\n",
      "Epoch    3 Cost: 0.016770\n",
      "Epoch    3 Cost: 0.016576\n",
      "Epoch    3 Cost: 0.016366\n",
      "Epoch    3 Cost: 0.016158\n",
      "Epoch    3 Cost: 0.015966\n",
      "Epoch    3 Cost: 0.015772\n",
      "Epoch    3 Cost: 0.015577\n",
      "Epoch    3 Cost: 0.015395\n",
      "Epoch    3 Cost: 0.015208\n",
      "Epoch    3 Cost: 0.015028\n",
      "Epoch    3 Cost: 0.014854\n",
      "Epoch    3 Cost: 0.014676\n",
      "Epoch    3 Cost: 0.014501\n",
      "Epoch    3 Cost: 0.014338\n",
      "Epoch    3 Cost: 0.014169\n",
      "Epoch    3 Cost: 0.014005\n",
      "Epoch    3 Cost: 0.013845\n",
      "Epoch    3 Cost: 0.013686\n",
      "Epoch    3 Cost: 0.013529\n",
      "Epoch    3 Cost: 0.013375\n",
      "Epoch    3 Cost: 0.013227\n",
      "Epoch    3 Cost: 0.013077\n",
      "Epoch    3 Cost: 0.012930\n",
      "Epoch    3 Cost: 0.012791\n",
      "Epoch    3 Cost: 0.012650\n",
      "Epoch    3 Cost: 0.012508\n",
      "Epoch    3 Cost: 0.012371\n",
      "Epoch    3 Cost: 0.012235\n",
      "Epoch    4 Cost: 0.012105\n",
      "Epoch    4 Cost: 0.011973\n",
      "Epoch    4 Cost: 0.011845\n",
      "Epoch    4 Cost: 0.011718\n",
      "Epoch    4 Cost: 0.011592\n",
      "Epoch    4 Cost: 0.011471\n",
      "Epoch    4 Cost: 0.011349\n",
      "Epoch    4 Cost: 0.011233\n",
      "Epoch    4 Cost: 0.011112\n",
      "Epoch    4 Cost: 0.010996\n",
      "Epoch    4 Cost: 0.010887\n",
      "Epoch    4 Cost: 0.010770\n",
      "Epoch    4 Cost: 0.010658\n",
      "Epoch    4 Cost: 0.010550\n",
      "Epoch    4 Cost: 0.010441\n",
      "Epoch    4 Cost: 0.010335\n",
      "Epoch    4 Cost: 0.010231\n",
      "Epoch    4 Cost: 0.010128\n",
      "Epoch    4 Cost: 0.010026\n",
      "Epoch    4 Cost: 0.009924\n",
      "Epoch    4 Cost: 0.009827\n",
      "Epoch    4 Cost: 0.009730\n",
      "Epoch    4 Cost: 0.009635\n",
      "Epoch    4 Cost: 0.009542\n",
      "Epoch    4 Cost: 0.009446\n",
      "Epoch    4 Cost: 0.009356\n",
      "Epoch    4 Cost: 0.009264\n",
      "Epoch    4 Cost: 0.009175\n",
      "Epoch    4 Cost: 0.009088\n",
      "Epoch    4 Cost: 0.009001\n",
      "Epoch    4 Cost: 0.008915\n",
      "Epoch    4 Cost: 0.008832\n",
      "Epoch    4 Cost: 0.008748\n",
      "Epoch    4 Cost: 0.008664\n",
      "Epoch    4 Cost: 0.008586\n",
      "Epoch    4 Cost: 0.008503\n",
      "Epoch    4 Cost: 0.008427\n",
      "Epoch    4 Cost: 0.008349\n",
      "Epoch    4 Cost: 0.008274\n",
      "Epoch    4 Cost: 0.008196\n",
      "Epoch    4 Cost: 0.008122\n",
      "Epoch    4 Cost: 0.008049\n",
      "Epoch    4 Cost: 0.007976\n",
      "Epoch    4 Cost: 0.007905\n",
      "Epoch    4 Cost: 0.007834\n",
      "Epoch    5 Cost: 0.007766\n",
      "Epoch    5 Cost: 0.007696\n",
      "Epoch    5 Cost: 0.007627\n",
      "Epoch    5 Cost: 0.007560\n",
      "Epoch    5 Cost: 0.007493\n",
      "Epoch    5 Cost: 0.007428\n",
      "Epoch    5 Cost: 0.007364\n",
      "Epoch    5 Cost: 0.007301\n",
      "Epoch    5 Cost: 0.007236\n",
      "Epoch    5 Cost: 0.007176\n",
      "Epoch    5 Cost: 0.007114\n",
      "Epoch    5 Cost: 0.007053\n",
      "Epoch    5 Cost: 0.006994\n",
      "Epoch    5 Cost: 0.006936\n",
      "Epoch    5 Cost: 0.006876\n",
      "Epoch    5 Cost: 0.006819\n",
      "Epoch    5 Cost: 0.006762\n",
      "Epoch    5 Cost: 0.006707\n",
      "Epoch    5 Cost: 0.006651\n",
      "Epoch    5 Cost: 0.006596\n",
      "Epoch    5 Cost: 0.006542\n",
      "Epoch    5 Cost: 0.006489\n",
      "Epoch    5 Cost: 0.006437\n",
      "Epoch    5 Cost: 0.006384\n",
      "Epoch    5 Cost: 0.006333\n",
      "Epoch    5 Cost: 0.006282\n",
      "Epoch    5 Cost: 0.006231\n",
      "Epoch    5 Cost: 0.006182\n",
      "Epoch    5 Cost: 0.006133\n",
      "Epoch    5 Cost: 0.006085\n",
      "Epoch    5 Cost: 0.006036\n",
      "Epoch    5 Cost: 0.005990\n",
      "Epoch    5 Cost: 0.005944\n",
      "Epoch    5 Cost: 0.005897\n",
      "Epoch    5 Cost: 0.005851\n",
      "Epoch    5 Cost: 0.005807\n",
      "Epoch    5 Cost: 0.005762\n",
      "Epoch    5 Cost: 0.005719\n",
      "Epoch    5 Cost: 0.005675\n",
      "Epoch    5 Cost: 0.005631\n",
      "Epoch    5 Cost: 0.005590\n",
      "Epoch    5 Cost: 0.005547\n",
      "Epoch    5 Cost: 0.005505\n",
      "Epoch    5 Cost: 0.005465\n",
      "Epoch    5 Cost: 0.005424\n",
      "Epoch    6 Cost: 0.005384\n",
      "Epoch    6 Cost: 0.005344\n",
      "Epoch    6 Cost: 0.005306\n",
      "Epoch    6 Cost: 0.005267\n",
      "Epoch    6 Cost: 0.005228\n",
      "Epoch    6 Cost: 0.005190\n",
      "Epoch    6 Cost: 0.005152\n",
      "Epoch    6 Cost: 0.005115\n",
      "Epoch    6 Cost: 0.005078\n",
      "Epoch    6 Cost: 0.005043\n",
      "Epoch    6 Cost: 0.005007\n",
      "Epoch    6 Cost: 0.004972\n",
      "Epoch    6 Cost: 0.004935\n",
      "Epoch    6 Cost: 0.004901\n",
      "Epoch    6 Cost: 0.004866\n",
      "Epoch    6 Cost: 0.004832\n",
      "Epoch    6 Cost: 0.004798\n",
      "Epoch    6 Cost: 0.004765\n",
      "Epoch    6 Cost: 0.004731\n",
      "Epoch    6 Cost: 0.004698\n",
      "Epoch    6 Cost: 0.004667\n",
      "Epoch    6 Cost: 0.004634\n",
      "Epoch    6 Cost: 0.004603\n",
      "Epoch    6 Cost: 0.004571\n",
      "Epoch    6 Cost: 0.004539\n",
      "Epoch    6 Cost: 0.004509\n",
      "Epoch    6 Cost: 0.004478\n",
      "Epoch    6 Cost: 0.004448\n",
      "Epoch    6 Cost: 0.004418\n",
      "Epoch    6 Cost: 0.004388\n",
      "Epoch    6 Cost: 0.004358\n",
      "Epoch    6 Cost: 0.004329\n",
      "Epoch    6 Cost: 0.004301\n",
      "Epoch    6 Cost: 0.004272\n",
      "Epoch    6 Cost: 0.004244\n",
      "Epoch    6 Cost: 0.004216\n",
      "Epoch    6 Cost: 0.004189\n",
      "Epoch    6 Cost: 0.004161\n",
      "Epoch    6 Cost: 0.004135\n",
      "Epoch    6 Cost: 0.004108\n",
      "Epoch    6 Cost: 0.004081\n",
      "Epoch    6 Cost: 0.004056\n",
      "Epoch    6 Cost: 0.004029\n",
      "Epoch    6 Cost: 0.004003\n",
      "Epoch    6 Cost: 0.003979\n",
      "Epoch    7 Cost: 0.003953\n",
      "Epoch    7 Cost: 0.003927\n",
      "Epoch    7 Cost: 0.003904\n",
      "Epoch    7 Cost: 0.003879\n",
      "Epoch    7 Cost: 0.003854\n",
      "Epoch    7 Cost: 0.003831\n",
      "Epoch    7 Cost: 0.003806\n",
      "Epoch    7 Cost: 0.003783\n",
      "Epoch    7 Cost: 0.003759\n",
      "Epoch    7 Cost: 0.003736\n",
      "Epoch    7 Cost: 0.003713\n",
      "Epoch    7 Cost: 0.003691\n",
      "Epoch    7 Cost: 0.003668\n",
      "Epoch    7 Cost: 0.003645\n",
      "Epoch    7 Cost: 0.003623\n",
      "Epoch    7 Cost: 0.003601\n",
      "Epoch    7 Cost: 0.003580\n",
      "Epoch    7 Cost: 0.003559\n",
      "Epoch    7 Cost: 0.003536\n",
      "Epoch    7 Cost: 0.003517\n",
      "Epoch    7 Cost: 0.003497\n",
      "Epoch    7 Cost: 0.003474\n",
      "Epoch    7 Cost: 0.003454\n",
      "Epoch    7 Cost: 0.003435\n",
      "Epoch    7 Cost: 0.003414\n",
      "Epoch    7 Cost: 0.003393\n",
      "Epoch    7 Cost: 0.003373\n",
      "Epoch    7 Cost: 0.003354\n",
      "Epoch    7 Cost: 0.003335\n",
      "Epoch    7 Cost: 0.003315\n",
      "Epoch    7 Cost: 0.003295\n",
      "Epoch    7 Cost: 0.003277\n",
      "Epoch    7 Cost: 0.003259\n",
      "Epoch    7 Cost: 0.003239\n",
      "Epoch    7 Cost: 0.003220\n",
      "Epoch    7 Cost: 0.003203\n",
      "Epoch    7 Cost: 0.003184\n",
      "Epoch    7 Cost: 0.003165\n",
      "Epoch    7 Cost: 0.003148\n",
      "Epoch    7 Cost: 0.003130\n",
      "Epoch    7 Cost: 0.003111\n",
      "Epoch    7 Cost: 0.003094\n",
      "Epoch    7 Cost: 0.003077\n",
      "Epoch    7 Cost: 0.003059\n",
      "Epoch    7 Cost: 0.003042\n",
      "Epoch    8 Cost: 0.003025\n",
      "Epoch    8 Cost: 0.003008\n",
      "Epoch    8 Cost: 0.002991\n",
      "Epoch    8 Cost: 0.002975\n",
      "Epoch    8 Cost: 0.002958\n",
      "Epoch    8 Cost: 0.002942\n",
      "Epoch    8 Cost: 0.002926\n",
      "Epoch    8 Cost: 0.002911\n",
      "Epoch    8 Cost: 0.002894\n",
      "Epoch    8 Cost: 0.002879\n",
      "Epoch    8 Cost: 0.002864\n",
      "Epoch    8 Cost: 0.002847\n",
      "Epoch    8 Cost: 0.002833\n",
      "Epoch    8 Cost: 0.002818\n",
      "Epoch    8 Cost: 0.002801\n",
      "Epoch    8 Cost: 0.002788\n",
      "Epoch    8 Cost: 0.002773\n",
      "Epoch    8 Cost: 0.002758\n",
      "Epoch    8 Cost: 0.002743\n",
      "Epoch    8 Cost: 0.002729\n",
      "Epoch    8 Cost: 0.002715\n",
      "Epoch    8 Cost: 0.002701\n",
      "Epoch    8 Cost: 0.002686\n",
      "Epoch    8 Cost: 0.002672\n",
      "Epoch    8 Cost: 0.002658\n",
      "Epoch    8 Cost: 0.002644\n",
      "Epoch    8 Cost: 0.002630\n",
      "Epoch    8 Cost: 0.002617\n",
      "Epoch    8 Cost: 0.002603\n",
      "Epoch    8 Cost: 0.002590\n",
      "Epoch    8 Cost: 0.002577\n",
      "Epoch    8 Cost: 0.002563\n",
      "Epoch    8 Cost: 0.002551\n",
      "Epoch    8 Cost: 0.002538\n",
      "Epoch    8 Cost: 0.002524\n",
      "Epoch    8 Cost: 0.002512\n",
      "Epoch    8 Cost: 0.002500\n",
      "Epoch    8 Cost: 0.002487\n",
      "Epoch    8 Cost: 0.002474\n",
      "Epoch    8 Cost: 0.002462\n",
      "Epoch    8 Cost: 0.002450\n",
      "Epoch    8 Cost: 0.002437\n",
      "Epoch    8 Cost: 0.002425\n",
      "Epoch    8 Cost: 0.002413\n",
      "Epoch    8 Cost: 0.002402\n",
      "Epoch    9 Cost: 0.002390\n",
      "Epoch    9 Cost: 0.002378\n",
      "Epoch    9 Cost: 0.002367\n",
      "Epoch    9 Cost: 0.002355\n",
      "Epoch    9 Cost: 0.002343\n",
      "Epoch    9 Cost: 0.002332\n",
      "Epoch    9 Cost: 0.002321\n",
      "Epoch    9 Cost: 0.002310\n",
      "Epoch    9 Cost: 0.002299\n",
      "Epoch    9 Cost: 0.002288\n",
      "Epoch    9 Cost: 0.002277\n",
      "Epoch    9 Cost: 0.002266\n",
      "Epoch    9 Cost: 0.002255\n",
      "Epoch    9 Cost: 0.002244\n",
      "Epoch    9 Cost: 0.002234\n",
      "Epoch    9 Cost: 0.002223\n",
      "Epoch    9 Cost: 0.002213\n",
      "Epoch    9 Cost: 0.002202\n",
      "Epoch    9 Cost: 0.002192\n",
      "Epoch    9 Cost: 0.002182\n",
      "Epoch    9 Cost: 0.002171\n",
      "Epoch    9 Cost: 0.002161\n",
      "Epoch    9 Cost: 0.002151\n",
      "Epoch    9 Cost: 0.002141\n",
      "Epoch    9 Cost: 0.002131\n",
      "Epoch    9 Cost: 0.002121\n",
      "Epoch    9 Cost: 0.002111\n",
      "Epoch    9 Cost: 0.002101\n",
      "Epoch    9 Cost: 0.002091\n",
      "Epoch    9 Cost: 0.002081\n",
      "Epoch    9 Cost: 0.002072\n",
      "Epoch    9 Cost: 0.002062\n",
      "Epoch    9 Cost: 0.002053\n",
      "Epoch    9 Cost: 0.002043\n",
      "Epoch    9 Cost: 0.002034\n",
      "Epoch    9 Cost: 0.002025\n",
      "Epoch    9 Cost: 0.002015\n",
      "Epoch    9 Cost: 0.002006\n",
      "Epoch    9 Cost: 0.001997\n",
      "Epoch    9 Cost: 0.001988\n",
      "Epoch    9 Cost: 0.001979\n",
      "Epoch    9 Cost: 0.001970\n",
      "Epoch    9 Cost: 0.001961\n",
      "Epoch    9 Cost: 0.001952\n",
      "Epoch    9 Cost: 0.001943\n",
      "Epoch   10 Cost: 0.001934\n",
      "Epoch   10 Cost: 0.001926\n",
      "Epoch   10 Cost: 0.001917\n",
      "Epoch   10 Cost: 0.001909\n",
      "Epoch   10 Cost: 0.001900\n",
      "Epoch   10 Cost: 0.001891\n",
      "Epoch   10 Cost: 0.001883\n",
      "Epoch   10 Cost: 0.001875\n",
      "Epoch   10 Cost: 0.001866\n",
      "Epoch   10 Cost: 0.001858\n",
      "Epoch   10 Cost: 0.001850\n",
      "Epoch   10 Cost: 0.001842\n",
      "Epoch   10 Cost: 0.001833\n",
      "Epoch   10 Cost: 0.001826\n",
      "Epoch   10 Cost: 0.001818\n",
      "Epoch   10 Cost: 0.001809\n",
      "Epoch   10 Cost: 0.001802\n",
      "Epoch   10 Cost: 0.001794\n",
      "Epoch   10 Cost: 0.001786\n",
      "Epoch   10 Cost: 0.001778\n",
      "Epoch   10 Cost: 0.001771\n",
      "Epoch   10 Cost: 0.001763\n",
      "Epoch   10 Cost: 0.001756\n",
      "Epoch   10 Cost: 0.001748\n",
      "Epoch   10 Cost: 0.001741\n",
      "Epoch   10 Cost: 0.001733\n",
      "Epoch   10 Cost: 0.001726\n",
      "Epoch   10 Cost: 0.001719\n",
      "Epoch   10 Cost: 0.001711\n",
      "Epoch   10 Cost: 0.001704\n",
      "Epoch   10 Cost: 0.001697\n",
      "Epoch   10 Cost: 0.001690\n",
      "Epoch   10 Cost: 0.001683\n",
      "Epoch   10 Cost: 0.001676\n",
      "Epoch   10 Cost: 0.001669\n",
      "Epoch   10 Cost: 0.001662\n",
      "Epoch   10 Cost: 0.001655\n",
      "Epoch   10 Cost: 0.001648\n",
      "Epoch   10 Cost: 0.001641\n",
      "Epoch   10 Cost: 0.001634\n",
      "Epoch   10 Cost: 0.001628\n",
      "Epoch   10 Cost: 0.001621\n",
      "Epoch   10 Cost: 0.001615\n",
      "Epoch   10 Cost: 0.001608\n",
      "Epoch   10 Cost: 0.001601\n",
      "Epoch   11 Cost: 0.001595\n",
      "Epoch   11 Cost: 0.001588\n",
      "Epoch   11 Cost: 0.001582\n",
      "Epoch   11 Cost: 0.001575\n",
      "Epoch   11 Cost: 0.001569\n",
      "Epoch   11 Cost: 0.001563\n",
      "Epoch   11 Cost: 0.001557\n",
      "Epoch   11 Cost: 0.001550\n",
      "Epoch   11 Cost: 0.001544\n",
      "Epoch   11 Cost: 0.001538\n",
      "Epoch   11 Cost: 0.001532\n",
      "Epoch   11 Cost: 0.001526\n",
      "Epoch   11 Cost: 0.001520\n",
      "Epoch   11 Cost: 0.001514\n",
      "Epoch   11 Cost: 0.001508\n",
      "Epoch   11 Cost: 0.001502\n",
      "Epoch   11 Cost: 0.001496\n",
      "Epoch   11 Cost: 0.001490\n",
      "Epoch   11 Cost: 0.001484\n",
      "Epoch   11 Cost: 0.001479\n",
      "Epoch   11 Cost: 0.001473\n",
      "Epoch   11 Cost: 0.001467\n",
      "Epoch   11 Cost: 0.001461\n",
      "Epoch   11 Cost: 0.001456\n",
      "Epoch   11 Cost: 0.001450\n",
      "Epoch   11 Cost: 0.001445\n",
      "Epoch   11 Cost: 0.001439\n",
      "Epoch   11 Cost: 0.001433\n",
      "Epoch   11 Cost: 0.001428\n",
      "Epoch   11 Cost: 0.001422\n",
      "Epoch   11 Cost: 0.001417\n",
      "Epoch   11 Cost: 0.001412\n",
      "Epoch   11 Cost: 0.001406\n",
      "Epoch   11 Cost: 0.001401\n",
      "Epoch   11 Cost: 0.001396\n",
      "Epoch   11 Cost: 0.001390\n",
      "Epoch   11 Cost: 0.001385\n",
      "Epoch   11 Cost: 0.001380\n",
      "Epoch   11 Cost: 0.001375\n",
      "Epoch   11 Cost: 0.001370\n",
      "Epoch   11 Cost: 0.001365\n",
      "Epoch   11 Cost: 0.001360\n",
      "Epoch   11 Cost: 0.001355\n",
      "Epoch   11 Cost: 0.001350\n",
      "Epoch   11 Cost: 0.001345\n",
      "Epoch   12 Cost: 0.001340\n",
      "Epoch   12 Cost: 0.001335\n",
      "Epoch   12 Cost: 0.001330\n",
      "Epoch   12 Cost: 0.001325\n",
      "Epoch   12 Cost: 0.001320\n",
      "Epoch   12 Cost: 0.001315\n",
      "Epoch   12 Cost: 0.001311\n",
      "Epoch   12 Cost: 0.001306\n",
      "Epoch   12 Cost: 0.001301\n",
      "Epoch   12 Cost: 0.001297\n",
      "Epoch   12 Cost: 0.001292\n",
      "Epoch   12 Cost: 0.001287\n",
      "Epoch   12 Cost: 0.001283\n",
      "Epoch   12 Cost: 0.001278\n",
      "Epoch   12 Cost: 0.001273\n",
      "Epoch   12 Cost: 0.001269\n",
      "Epoch   12 Cost: 0.001264\n",
      "Epoch   12 Cost: 0.001260\n",
      "Epoch   12 Cost: 0.001255\n",
      "Epoch   12 Cost: 0.001251\n",
      "Epoch   12 Cost: 0.001247\n",
      "Epoch   12 Cost: 0.001242\n",
      "Epoch   12 Cost: 0.001238\n",
      "Epoch   12 Cost: 0.001233\n",
      "Epoch   12 Cost: 0.001229\n",
      "Epoch   12 Cost: 0.001225\n",
      "Epoch   12 Cost: 0.001221\n",
      "Epoch   12 Cost: 0.001216\n",
      "Epoch   12 Cost: 0.001212\n",
      "Epoch   12 Cost: 0.001208\n",
      "Epoch   12 Cost: 0.001204\n",
      "Epoch   12 Cost: 0.001199\n",
      "Epoch   12 Cost: 0.001195\n",
      "Epoch   12 Cost: 0.001191\n",
      "Epoch   12 Cost: 0.001187\n",
      "Epoch   12 Cost: 0.001183\n",
      "Epoch   12 Cost: 0.001179\n",
      "Epoch   12 Cost: 0.001175\n",
      "Epoch   12 Cost: 0.001171\n",
      "Epoch   12 Cost: 0.001167\n",
      "Epoch   12 Cost: 0.001163\n",
      "Epoch   12 Cost: 0.001159\n",
      "Epoch   12 Cost: 0.001155\n",
      "Epoch   12 Cost: 0.001151\n",
      "Epoch   12 Cost: 0.001147\n",
      "Epoch   13 Cost: 0.001143\n",
      "Epoch   13 Cost: 0.001139\n",
      "Epoch   13 Cost: 0.001136\n",
      "Epoch   13 Cost: 0.001132\n",
      "Epoch   13 Cost: 0.001128\n",
      "Epoch   13 Cost: 0.001124\n",
      "Epoch   13 Cost: 0.001120\n",
      "Epoch   13 Cost: 0.001117\n",
      "Epoch   13 Cost: 0.001113\n",
      "Epoch   13 Cost: 0.001109\n",
      "Epoch   13 Cost: 0.001105\n",
      "Epoch   13 Cost: 0.001102\n",
      "Epoch   13 Cost: 0.001098\n",
      "Epoch   13 Cost: 0.001094\n",
      "Epoch   13 Cost: 0.001091\n",
      "Epoch   13 Cost: 0.001087\n",
      "Epoch   13 Cost: 0.001084\n",
      "Epoch   13 Cost: 0.001080\n",
      "Epoch   13 Cost: 0.001077\n",
      "Epoch   13 Cost: 0.001073\n",
      "Epoch   13 Cost: 0.001070\n",
      "Epoch   13 Cost: 0.001066\n",
      "Epoch   13 Cost: 0.001063\n",
      "Epoch   13 Cost: 0.001059\n",
      "Epoch   13 Cost: 0.001056\n",
      "Epoch   13 Cost: 0.001052\n",
      "Epoch   13 Cost: 0.001049\n",
      "Epoch   13 Cost: 0.001045\n",
      "Epoch   13 Cost: 0.001042\n",
      "Epoch   13 Cost: 0.001039\n",
      "Epoch   13 Cost: 0.001035\n",
      "Epoch   13 Cost: 0.001032\n",
      "Epoch   13 Cost: 0.001029\n",
      "Epoch   13 Cost: 0.001025\n",
      "Epoch   13 Cost: 0.001022\n",
      "Epoch   13 Cost: 0.001019\n",
      "Epoch   13 Cost: 0.001016\n",
      "Epoch   13 Cost: 0.001012\n",
      "Epoch   13 Cost: 0.001009\n",
      "Epoch   13 Cost: 0.001006\n",
      "Epoch   13 Cost: 0.001003\n",
      "Epoch   13 Cost: 0.000999\n",
      "Epoch   13 Cost: 0.000996\n",
      "Epoch   13 Cost: 0.000993\n",
      "Epoch   13 Cost: 0.000990\n",
      "Epoch   14 Cost: 0.000987\n",
      "Epoch   14 Cost: 0.000984\n",
      "Epoch   14 Cost: 0.000981\n",
      "Epoch   14 Cost: 0.000978\n",
      "Epoch   14 Cost: 0.000975\n",
      "Epoch   14 Cost: 0.000972\n",
      "Epoch   14 Cost: 0.000968\n",
      "Epoch   14 Cost: 0.000965\n",
      "Epoch   14 Cost: 0.000962\n",
      "Epoch   14 Cost: 0.000960\n",
      "Epoch   14 Cost: 0.000957\n",
      "Epoch   14 Cost: 0.000954\n",
      "Epoch   14 Cost: 0.000951\n",
      "Epoch   14 Cost: 0.000948\n",
      "Epoch   14 Cost: 0.000945\n",
      "Epoch   14 Cost: 0.000942\n",
      "Epoch   14 Cost: 0.000939\n",
      "Epoch   14 Cost: 0.000936\n",
      "Epoch   14 Cost: 0.000933\n",
      "Epoch   14 Cost: 0.000930\n",
      "Epoch   14 Cost: 0.000928\n",
      "Epoch   14 Cost: 0.000925\n",
      "Epoch   14 Cost: 0.000922\n",
      "Epoch   14 Cost: 0.000919\n",
      "Epoch   14 Cost: 0.000916\n",
      "Epoch   14 Cost: 0.000914\n",
      "Epoch   14 Cost: 0.000911\n",
      "Epoch   14 Cost: 0.000908\n",
      "Epoch   14 Cost: 0.000905\n",
      "Epoch   14 Cost: 0.000903\n",
      "Epoch   14 Cost: 0.000900\n",
      "Epoch   14 Cost: 0.000897\n",
      "Epoch   14 Cost: 0.000895\n",
      "Epoch   14 Cost: 0.000892\n",
      "Epoch   14 Cost: 0.000889\n",
      "Epoch   14 Cost: 0.000886\n",
      "Epoch   14 Cost: 0.000884\n",
      "Epoch   14 Cost: 0.000881\n",
      "Epoch   14 Cost: 0.000879\n",
      "Epoch   14 Cost: 0.000876\n",
      "Epoch   14 Cost: 0.000873\n",
      "Epoch   14 Cost: 0.000871\n",
      "Epoch   14 Cost: 0.000868\n",
      "Epoch   14 Cost: 0.000866\n",
      "Epoch   14 Cost: 0.000863\n",
      "Epoch   15 Cost: 0.000861\n",
      "Epoch   15 Cost: 0.000858\n",
      "Epoch   15 Cost: 0.000855\n",
      "Epoch   15 Cost: 0.000853\n",
      "Epoch   15 Cost: 0.000850\n",
      "Epoch   15 Cost: 0.000848\n",
      "Epoch   15 Cost: 0.000846\n",
      "Epoch   15 Cost: 0.000843\n",
      "Epoch   15 Cost: 0.000841\n",
      "Epoch   15 Cost: 0.000838\n",
      "Epoch   15 Cost: 0.000836\n",
      "Epoch   15 Cost: 0.000833\n",
      "Epoch   15 Cost: 0.000831\n",
      "Epoch   15 Cost: 0.000829\n",
      "Epoch   15 Cost: 0.000826\n",
      "Epoch   15 Cost: 0.000824\n",
      "Epoch   15 Cost: 0.000821\n",
      "Epoch   15 Cost: 0.000819\n",
      "Epoch   15 Cost: 0.000817\n",
      "Epoch   15 Cost: 0.000814\n",
      "Epoch   15 Cost: 0.000812\n",
      "Epoch   15 Cost: 0.000810\n",
      "Epoch   15 Cost: 0.000807\n",
      "Epoch   15 Cost: 0.000805\n",
      "Epoch   15 Cost: 0.000803\n",
      "Epoch   15 Cost: 0.000801\n",
      "Epoch   15 Cost: 0.000798\n",
      "Epoch   15 Cost: 0.000796\n",
      "Epoch   15 Cost: 0.000794\n",
      "Epoch   15 Cost: 0.000792\n",
      "Epoch   15 Cost: 0.000789\n",
      "Epoch   15 Cost: 0.000787\n",
      "Epoch   15 Cost: 0.000785\n",
      "Epoch   15 Cost: 0.000783\n",
      "Epoch   15 Cost: 0.000781\n",
      "Epoch   15 Cost: 0.000778\n",
      "Epoch   15 Cost: 0.000776\n",
      "Epoch   15 Cost: 0.000774\n",
      "Epoch   15 Cost: 0.000772\n",
      "Epoch   15 Cost: 0.000770\n",
      "Epoch   15 Cost: 0.000768\n",
      "Epoch   15 Cost: 0.000765\n",
      "Epoch   15 Cost: 0.000763\n",
      "Epoch   15 Cost: 0.000761\n",
      "Epoch   15 Cost: 0.000759\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs+1):\n",
    "    for batch_idx, samples in enumerate(dataloader):\n",
    "        \n",
    "        x =x.float()\n",
    "        t_true = y_train.long()\n",
    "        y_true = torch.argmax(t_true,dim=0)\n",
    "\n",
    "        \n",
    "        prediction = model(x_train)\n",
    "\n",
    "        cost = F.cross_entropy(prediction,y_train)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        cost.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        \n",
    "        print('Epoch {:4d} Cost: {:.6f}'.format(\n",
    "            epoch, cost.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(x_test[0])\n",
    "y_test[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch.utils' has no attribute 'Dataset'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mcustom\u001b[39;00m(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataset\u001b[49m):\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m,x,y):\n\u001b[1;32m      6\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor(mnist\u001b[38;5;241m.\u001b[39mdata)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'torch.utils' has no attribute 'Dataset'"
     ]
    }
   ],
   "source": [
    "import torch.utils\n",
    "\n",
    "\n",
    "class custom(torch.utils.Dataset):\n",
    "    def __init__(self,x,y):\n",
    "        self.x = torch.FloatTensor(mnist.data)\n",
    "        self.y = torch.FloatTensor(mnist.target)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        return self.x[index], self.y[index]\n",
    "    \n",
    "dataset = custom(mnist.data,mnist.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(2.3484, grad_fn=<NllLossBackward0>)\n",
      "0 tensor(2.3379, grad_fn=<NllLossBackward0>)\n",
      "0 tensor(2.3181, grad_fn=<NllLossBackward0>)\n",
      "0 tensor(2.3389, grad_fn=<NllLossBackward0>)\n",
      "0 tensor(2.3402, grad_fn=<NllLossBackward0>)\n",
      "0 tensor(2.3526, grad_fn=<NllLossBackward0>)\n",
      "0 tensor(2.3608, grad_fn=<NllLossBackward0>)\n",
      "0 tensor(2.3535, grad_fn=<NllLossBackward0>)\n",
      "0 tensor(2.3345, grad_fn=<NllLossBackward0>)\n",
      "0 tensor(2.3454, grad_fn=<NllLossBackward0>)\n",
      "0 tensor(2.3315, grad_fn=<NllLossBackward0>)\n",
      "0 tensor(2.3315, grad_fn=<NllLossBackward0>)\n",
      "0 tensor(2.3381, grad_fn=<NllLossBackward0>)\n",
      "0 tensor(2.3514, grad_fn=<NllLossBackward0>)\n",
      "0 tensor(2.3345, grad_fn=<NllLossBackward0>)\n",
      "0 tensor(2.3347, grad_fn=<NllLossBackward0>)\n",
      "0 tensor(2.3500, grad_fn=<NllLossBackward0>)\n",
      "0 tensor(2.3187, grad_fn=<NllLossBackward0>)\n",
      "0 tensor(2.3119, grad_fn=<NllLossBackward0>)\n",
      "0 tensor(2.3044, grad_fn=<NllLossBackward0>)\n",
      "0 tensor(2.3343, grad_fn=<NllLossBackward0>)\n",
      "0 tensor(2.2924, grad_fn=<NllLossBackward0>)\n",
      "0 tensor(2.3251, grad_fn=<NllLossBackward0>)\n",
      "0 tensor(2.3350, grad_fn=<NllLossBackward0>)\n",
      "0 tensor(2.3677, grad_fn=<NllLossBackward0>)\n",
      "0 tensor(2.3167, grad_fn=<NllLossBackward0>)\n",
      "0 tensor(2.2899, grad_fn=<NllLossBackward0>)\n",
      "0 tensor(2.3303, grad_fn=<NllLossBackward0>)\n",
      "0 tensor(2.3294, grad_fn=<NllLossBackward0>)\n",
      "0 tensor(2.3524, grad_fn=<NllLossBackward0>)\n",
      "0 tensor(2.3263, grad_fn=<NllLossBackward0>)\n",
      "0 tensor(2.3045, grad_fn=<NllLossBackward0>)\n",
      "0 tensor(2.3435, grad_fn=<NllLossBackward0>)\n",
      "0 tensor(2.3111, grad_fn=<NllLossBackward0>)\n",
      "0 tensor(2.3174, grad_fn=<NllLossBackward0>)\n",
      "0 tensor(2.3016, grad_fn=<NllLossBackward0>)\n",
      "0 tensor(2.3127, grad_fn=<NllLossBackward0>)\n",
      "0 tensor(2.3174, grad_fn=<NllLossBackward0>)\n",
      "0 tensor(2.2272, grad_fn=<NllLossBackward0>)\n",
      "0 tensor(2.2898, grad_fn=<NllLossBackward0>)\n",
      "0 tensor(2.2881, grad_fn=<NllLossBackward0>)\n",
      "0 tensor(2.2507, grad_fn=<NllLossBackward0>)\n",
      "0 tensor(2.3141, grad_fn=<NllLossBackward0>)\n",
      "0 tensor(2.3154, grad_fn=<NllLossBackward0>)\n",
      "0 tensor(2.2896, grad_fn=<NllLossBackward0>)\n",
      "10 tensor(1.8981, grad_fn=<NllLossBackward0>)\n",
      "10 tensor(2.0792, grad_fn=<NllLossBackward0>)\n",
      "10 tensor(2.0626, grad_fn=<NllLossBackward0>)\n",
      "10 tensor(2.1333, grad_fn=<NllLossBackward0>)\n",
      "10 tensor(1.9543, grad_fn=<NllLossBackward0>)\n",
      "10 tensor(2.0691, grad_fn=<NllLossBackward0>)\n",
      "10 tensor(2.1007, grad_fn=<NllLossBackward0>)\n",
      "10 tensor(1.9724, grad_fn=<NllLossBackward0>)\n",
      "10 tensor(2.1852, grad_fn=<NllLossBackward0>)\n",
      "10 tensor(1.9909, grad_fn=<NllLossBackward0>)\n",
      "10 tensor(2.0459, grad_fn=<NllLossBackward0>)\n",
      "10 tensor(1.9834, grad_fn=<NllLossBackward0>)\n",
      "10 tensor(2.0733, grad_fn=<NllLossBackward0>)\n",
      "10 tensor(2.0238, grad_fn=<NllLossBackward0>)\n",
      "10 tensor(1.9090, grad_fn=<NllLossBackward0>)\n",
      "10 tensor(1.9839, grad_fn=<NllLossBackward0>)\n",
      "10 tensor(2.0104, grad_fn=<NllLossBackward0>)\n",
      "10 tensor(2.1332, grad_fn=<NllLossBackward0>)\n",
      "10 tensor(2.0564, grad_fn=<NllLossBackward0>)\n",
      "10 tensor(1.9956, grad_fn=<NllLossBackward0>)\n",
      "10 tensor(1.8995, grad_fn=<NllLossBackward0>)\n",
      "10 tensor(1.9759, grad_fn=<NllLossBackward0>)\n",
      "10 tensor(1.9045, grad_fn=<NllLossBackward0>)\n",
      "10 tensor(2.0510, grad_fn=<NllLossBackward0>)\n",
      "10 tensor(2.0148, grad_fn=<NllLossBackward0>)\n",
      "10 tensor(1.8711, grad_fn=<NllLossBackward0>)\n",
      "10 tensor(1.9720, grad_fn=<NllLossBackward0>)\n",
      "10 tensor(2.0898, grad_fn=<NllLossBackward0>)\n",
      "10 tensor(2.0152, grad_fn=<NllLossBackward0>)\n",
      "10 tensor(2.0321, grad_fn=<NllLossBackward0>)\n",
      "10 tensor(2.0493, grad_fn=<NllLossBackward0>)\n",
      "10 tensor(2.0188, grad_fn=<NllLossBackward0>)\n",
      "10 tensor(1.9812, grad_fn=<NllLossBackward0>)\n",
      "10 tensor(2.1389, grad_fn=<NllLossBackward0>)\n",
      "10 tensor(2.0570, grad_fn=<NllLossBackward0>)\n",
      "10 tensor(2.0073, grad_fn=<NllLossBackward0>)\n",
      "10 tensor(1.9549, grad_fn=<NllLossBackward0>)\n",
      "10 tensor(1.9323, grad_fn=<NllLossBackward0>)\n",
      "10 tensor(2.0413, grad_fn=<NllLossBackward0>)\n",
      "10 tensor(2.0257, grad_fn=<NllLossBackward0>)\n",
      "10 tensor(2.0089, grad_fn=<NllLossBackward0>)\n",
      "10 tensor(1.9747, grad_fn=<NllLossBackward0>)\n",
      "10 tensor(1.9072, grad_fn=<NllLossBackward0>)\n",
      "10 tensor(2.0675, grad_fn=<NllLossBackward0>)\n",
      "10 tensor(2.0521, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class fun2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(64,32)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(32,10)\n",
    "        self.softmax = nn.Softmax(dim = 1)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.linear(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "model2 = fun2()\n",
    "\n",
    "optimizer2 = torch.optim.SGD(model2.parameters(),lr=0.005)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "epoch = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for data,target in dataloader:\n",
    "\n",
    "        output = model2(data)\n",
    "\n",
    "        loss = criterion(output,target)\n",
    "\n",
    "        optimizer2.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer2.step()\n",
    "\n",
    "        if epoch%10 == 0:\n",
    "            print(epoch,loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1257,)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train,x_test,y_train,y_test = train_test_split(mnist.data,mnist.target,test_size=0.3,random_state=42,stratify=mnist.target)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.datasets import load_digits\n",
    "\n",
    "mnist = load_digits()\n",
    "\n",
    "class customDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = torch.FloatTensor(x)\n",
    "        self.y = torch.LongTensor(y) # 오류\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]\n",
    "train_ds =customDataset(x_train,y_train)\n",
    "test_ds = customDataset(x_test,y_test)\n",
    "    \n",
    "dataset = customDataset(mnist.data, mnist.target)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils\n",
    "\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_ds,batch_size=32,shuffle=True)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_ds,batch_size=32,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "customModel(\n",
      "  (linear1): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (linear2): Linear(in_features=32, out_features=10, bias=True)\n",
      "  (relu): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class customModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(customModel, self).__init__()\n",
    "        self.linear1 = nn.Linear(64, 32)\n",
    "        self.linear2 = nn.Linear(32, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "    \n",
    "model = customModel()\n",
    "print(model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model2.parameters(), lr=0.005)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "epochs = 10\n",
    "\n",
    "def train(model,dataloader,optimizer,loss,epochs):\n",
    "    for epoch in range(epochs):\n",
    "        for data,target in dataloader:\n",
    "            y_pred = model2(data)\n",
    "\n",
    "            loss = loss_func(y_pred.detach().numpy(),target.detach().numpy())\n",
    "\n",
    "            acc= accuracy_score(target,y_pred.argmax())\n",
    "\n",
    "            optimizer2.zero_grad()\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer2.step()\n",
    "        \n",
    "        print(f\"epoch :{epoch}, Loss : {loss.item()} , acc : {acc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model,dataloader,optimizer,loss,epochs):\n",
    "    for epoch in range(epochs):\n",
    "        for data,target in dataloader:\n",
    "            y_pred = model2(data)\n",
    "\n",
    "            loss = loss_func(y_pred,target)\n",
    "        \n",
    "        print(f\"epoch :{epoch}, Loss : {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cross_entropy_loss(): argument 'input' (position 1) must be Tensor, not numpy.ndarray",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[64], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel2\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43moptimizer2\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[63], line 10\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, dataloader, optimizer, loss, epochs)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data,target \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[1;32m      8\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m model2(data)\n\u001b[0;32m---> 10\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_pred\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m     acc\u001b[38;5;241m=\u001b[39m accuracy_score(target,y_pred)\n\u001b[1;32m     14\u001b[0m     optimizer2\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/miniconda3/envs/on_1/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/on_1/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/on_1/lib/python3.11/site-packages/torch/nn/modules/loss.py:1188\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m-> 1188\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1189\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1190\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/on_1/lib/python3.11/site-packages/torch/nn/functional.py:3104\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3103\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3104\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: cross_entropy_loss(): argument 'input' (position 1) must be Tensor, not numpy.ndarray"
     ]
    }
   ],
   "source": [
    "train(model2,train_dataloader,optimizer2,criterion,epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch :0, Loss : 2.0325779914855957\n",
      "epoch :1, Loss : 2.0325779914855957\n",
      "epoch :2, Loss : 2.0325779914855957\n",
      "epoch :3, Loss : 2.0325779914855957\n",
      "epoch :4, Loss : 2.0325779914855957\n",
      "epoch :5, Loss : 2.0325779914855957\n",
      "epoch :6, Loss : 2.0325779914855957\n",
      "epoch :7, Loss : 2.0325779914855957\n",
      "epoch :8, Loss : 2.0325779914855957\n",
      "epoch :9, Loss : 2.0325779914855957\n"
     ]
    }
   ],
   "source": [
    "test(model2,test_dataloader,optimizer2,criterion,epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "on_1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
